{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c020700c",
   "metadata": {},
   "source": [
    "# Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270957ec",
   "metadata": {},
   "source": [
    "Das vorliegende Projekt zielt darauf ab, ein binäres Klassifikationsproblem im Bereich des Kreditwesens zu lösen. Dabei wird mittels Machine Learning Techniken versucht, anhand der Informationen von Kreditnehmern präzise Vorhersagen über die Zusage oder Absage von Krediten zu treffen, um Risiken der Kreditvergbern zu reduzieren.\n",
    "\n",
    "Der Bericht wird den gesamten Prozess des Projekts detailliert darlegen, beginnend mit der Vorbereitung und Erkundung der Daten bis hin zur Entwicklung und Auswertung des Modells. Es werden die angewandten Methoden, die erzielten Ergebnisse und die gezogenen Schlussfolgerungen ausführlich beschrieben, um ein umfassendes Verständnis für den Ansatz und die erreichten Ergebnisse zu vermitteln.\n",
    "\n",
    "*Link für den Gitlab-Ordner:* https://git.dhbw-stuttgart.de/wi22077/dataexploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ed29c",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af2bd9",
   "metadata": {},
   "source": [
    "Um die erforderlichen Bibliotheken zu installieren, bitte den folgenden Befehl ausführen:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f84495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score,roc_curve, auc, f1_score, make_scorer, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,learning_curve,GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4450cde",
   "metadata": {},
   "source": [
    "# Charakterisierung des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8bf2c",
   "metadata": {},
   "source": [
    "Der vorliegende Datensatz enthält Informationen über potenzielle Kreditnehmer und dient als Grundlage für die Vorhersage von Kreditzusagen oder -ablehnungen mittels eines binären Klassifikationsmodells. Der Datensatz besteht aus insgesamt 614 Zeilen und umfasst 13 Features, von denen 8 kategorischer und 4 kontinuierlicher Features sind. Zusätzlich gibt es einen Feature zur Identifizierung des Kredits (Loan_ID).\n",
    "\n",
    "Die kategorischen Features umfassen:\n",
    "- Geschlecht (Gender): Die Geschlechtsidentität des Kreditnehmers.\n",
    "- Verheiratet (Married): Der eheliche Status des Kreditnehmers.\n",
    "- Abhängige (Dependents): Die Anzahl der Familienmitglieder, die finanziell vom Kreditnehmer abhängig sind.\n",
    "- Bildung (Education): Erfasst, ob der Kreditnehmer eine höhere Abschlussausbildung abgeschlossen hat oder nicht.\n",
    "- Selbstständig (Self_Employed): Der Beschäftigungsstatus des Kreditnehmers.\n",
    "- Kreditgeschichte (Credit_History): Aufzeichnungen über die vorherige Kredithistorie des Kreditnehmers (0: schlechte Kredithistorie, 1: gute Kredithistorie).\n",
    "- Immobilienbereich (Property_Area): Die Lage des Eigentums des Kreditnehmers (ländlich, halb-urban, urban).\n",
    "- Kreditstatus (Loan_Status): Der Status des beantragten Kredits (Y: akzeptiert, N: nicht akzeptiert).\n",
    "\n",
    "Die kontinuierlichen Features umfassen:\n",
    "- ApplicantIncome: Das monatliche Einkommen des Antragstellers.\n",
    "- CoapplicantIncome: Das zusätzliche monatliche Einkommen des Mitbewerbers.\n",
    "- LoanAmount: Die Höhe des beantragten Kredits in Tausenden.\n",
    "- Loan_Amount_Term: Die Laufzeit des Kredits in Monaten.\n",
    "\n",
    "Die Kombination dieser Variablen bietet einen detailierten Einblick in die finanzielle Situation der Kreditnehmer sowie in ihre persönlichen und beruflichen Hintergründe. Dies ermöglicht es, potenzielle Muster und Zusammenhänge zu identifizieren, die die Kreditentscheidung beeinflussen könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data into DataFrame and display summary\n",
    "df = pd.read_csv(\"data/loan_dataset.csv\")\n",
    "df.info()  # Display DataFrame info\n",
    "df.head()  # Display first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b32fe6",
   "metadata": {},
   "source": [
    "Die Datenqualität des vorliegenden Datensatzes für das Prjekt weist einige wichtige Aspekte auf, die berücksichtigt werden müssen.\n",
    "\n",
    "Zunächst einmal gibt es einige NaN-Werte pro Zeile, insbesondere in den Spalten 'Gender', 'Married', 'Dependents', 'Self_Employed', 'LoanAmount', 'Loan_Amount_Term' und 'Credit_History'. Für Spalten mit kategorischen Werten im Objektformat werden die NaN-Werte durch den Modus der jeweiligen Spalte ersetzt, während für Spalten mit numerischen kontinuierlichen Werten der Median verwendet wird. Ein weiterer wichtiger Punkt betrifft die Spalte 'CoapplicantIncome', die auffällig viele 0-Werte aufweist. Dies deutet darauf hin, dass in vielen Fällen der Kreditnehmer der alleinige Verdienstträger ist, was etwa 44% der Fälle ausmacht. Dieser Merkmal könnte potenziell wichtige Informationen über die finanzielle Situation der Kreditnehmer liefern und sollte bei der Modellierung berücksichtigt werden. Desweiteren weist die Spalte 'Credit_History'  0-Werte auf, was auf einen negativen Kreditverlauf hinweist. Es ist wichtig zu beachten, dass diese Werte eigentlich kategorial sind und entweder einen guten Kreditverlauf (1) oder einen schlechten Kreditverlauf (0) repräsentieren. Daher müssen diese Werte entsprechend umgewandelt werden, um eine korrekte Interpretation zu ermöglichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fa8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through each column in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Counting the number of NaN (missing) values in the current column\n",
    "    null_count = (df[column].isna()).sum()\n",
    "    \n",
    "    # Counting the number of 0 values in the current column\n",
    "    zero_count = (df[column] == 0).sum()\n",
    "    \n",
    "    # Printing the counts of zero and NaN values for the current column\n",
    "    print(f\"Column '{column}': 0: {zero_count}, NaN: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bd1a7",
   "metadata": {},
   "source": [
    "Die Analyse der Werte in jeweiligen Spalten in unserem Datensatz zeigt eine gute Variation in den Kategorien, jedoch mit einigen fehlenden Daten in Schlüsselspalten wie 'Gender', 'Married', 'Dependents','Self_Employed', und 'Credit_History'. Diese Spalten umfassen sowohl kategorische als auch numerische Daten, die den Familienstand, das Geschlecht, die Abhängigen, den Beschäftigungsstatus und die Kredithistorie des Kreditnehmers darstellen. Die 'Credit_History'-Spalte, obwohl sie numerische Werte wie 0 und 1 enthält, wird als kategorische Variable behandelt, da sie lediglich zwei Zustände (gute oder schlechte Kredithistorie) repräsentiert. Während Spalten wie 'Education', 'Property_Area', und 'Loan_Status' vollständig sind, bedürfen andere einer sorgfältigen Behandlung der fehlenden Werte, um die Modellgenauigkeit zu verbessern. Zusätzlich wurde die erste Spalte 'Loan_ID' entfernt, da sie keine Relevanz für weitere Analysen oder Modellierungen hat und lediglich als Identifikator für einzelne Kredite diente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the first column 'Loan ID' since it's deemed unnecessary\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# Converts the 'Credit_History' column from float to object datatype since it's categorical\n",
    "df['Credit_History'] = df['Credit_History'].astype(str)\n",
    "\n",
    "# Iterates through columns with object datatype and prints their unique values\n",
    "for column in df.select_dtypes(include='object').columns:\n",
    "    # Extracts unique values of the current column\n",
    "    unique_values = df[column].unique()\n",
    "    \n",
    "    # Prints the unique values of the current column\n",
    "    print(f\"Unique values of column '{column}': {unique_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2593eff",
   "metadata": {},
   "source": [
    "Die Analyse der Klassenverteilung zeigt, dass der Datensatz eine gewisse Ungleichheit aufweist, wobei 422 Kredite als akzeptiert ('Y') und nur 192 Kredite als nicht akzeptiert ('N') gekennzeichnet sind. Diese Ungleichheit könnte potenziell Auswirkungen auf die Modellleistung haben, da das Modell möglicherweise dazu neigt, die überrepräsentierte Klasse besser zu lernen und die unterrepräsentierte Klasse zu vernachlässigen. Um dieses Problem anzugehen, könnten verschiedene Ansätze wie das Sampling von Daten (z. B. Oversampling der unterrepräsentierten Klasse oder Undersampling der überrepräsentierten Klasse) oder die Verwendung von Techniken wie dem Gewichten von Klassen in Betracht gezogen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7caadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verteilung der Klassen:\", Counter(df['Loan_Status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f6fd8",
   "metadata": {},
   "source": [
    "Die Spalten werden visualisiert, indem sie jeweils nach einzigartigen Werten gruppiert und anschließend nach dem Kreditstatus, also ob eine Person einen Kredit erhalten hat oder nicht, aufgeschlüsselt wurden.\n",
    "\n",
    "Die Analyse der Datenvisualisierung zeigt interessante Muster in Bezug auf die Genehmigung von Krediten basierend auf verschiedenen Merkmalen:\n",
    "- Die Mehrheit der Kreditnehmer sind männlich, und Männer haben eine höhere Genehmigungsrate für Kredite im Vergleich zu Frauen.\n",
    "- Verheiratete Personen stellen den größten Anteil der Kreditnehmer dar, und sie haben eine höhere Genehmigungsrate im Vergleich zu unverheirateten Personen.\n",
    "- Personen ohne Angehörige haben eine höhere Genehmigungsrate für Kredite.\n",
    "- Absolventen haben eine höhere Genehmigungsrate als Personen ohne Hochschulabschluss.\n",
    "- Selbstständige haben eine niedrigere Genehmigungsrate im Vergleich zu Angestellten.\n",
    "- Städtische Bewohner haben tendenziell eine leicht höhere Genehmigungsrate für Kredite im Vergleich zu ländlichen oder semiurbanen Bewohnern.\n",
    "- Personen mit einer positiven Kredithistorie haben eine deutlich höhere Genehmigungsrate im Vergleich zu Personen mit negativer oder unbekannter Kredithistorie.\n",
    "\n",
    "Die Histogramme und Boxplots der numerischen Spalten zeigen eine breite Streuung der Daten und das Vorhandensein von Ausreißern, was auf eine dringende Notwendigkeit der Bereinigung hinweist, um die Qualität der Modelle zu verbessern. Des Weiteren offenbart die Korrelationsmatrix  starke Zusammenhänge zwischen bestimmten Merkmalen, insbesondere zwischen der Kreditsumme und dem Einkommen des Kreditnehmers sowie zwischen Kreditzusage und Kredithistorie.\n",
    "\n",
    "Der Datensatz enthält reichhaltige Informationen, aber weist auch Herausforderungen in Bezug auf die Datenqualität und Ausreißer auf, die behoben werden müssen, um genaue Vorhersagen zu treffen und aussagekräftige Einsichten zu gewinnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b51995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust fig and font size\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "# Distribution of categorical variables\n",
    "plt.figure(figsize=(15, 30))  # Set the figure size for better visualization\n",
    "\n",
    "# Subplots for each categorical variable\n",
    "plt.subplot(4, 2, 1)\n",
    "sns.countplot(data=df, x='Gender', hue='Loan_Status')\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "sns.countplot(data=df, x='Married', hue='Loan_Status')\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "sns.countplot(data=df, x='Dependents', hue='Loan_Status')\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "sns.countplot(data=df, x='Education', hue='Loan_Status')\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "sns.countplot(data=df, x='Self_Employed', hue='Loan_Status')\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "sns.countplot(data=df, x='Property_Area', hue='Loan_Status')\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "sns.countplot(data=df, x='Credit_History', hue='Loan_Status')\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "sns.countplot(data=df, x='Loan_Status')  # Distribution of loan status itself\n",
    "\n",
    "plt.show()  # Show the plots\n",
    "\n",
    "# Histograms for numerical data\n",
    "plt.figure(figsize=(12, 15))  # Set the figure size\n",
    "\n",
    "# Subplots for each numerical variable\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df['ApplicantIncome'], bins=20, kde=True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(df['CoapplicantIncome'], bins=20, kde=True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df['LoanAmount'], bins=20, kde=True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(df['Loan_Amount_Term'], bins=20, kde=True)\n",
    "\n",
    "plt.show()  # Show the plots\n",
    "\n",
    "# Boxplots for outlier detection\n",
    "plt.figure(figsize=(12, 15))  # Set the figure size\n",
    "\n",
    "# Subplots for each numerical variable\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(data=df, x='Loan_Status', y='ApplicantIncome')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(data=df, x='Loan_Status', y='CoapplicantIncome')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=df, x='Loan_Status', y='LoanAmount')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(data=df, x='Loan_Status', y='Loan_Amount_Term')\n",
    "\n",
    "plt.show()  # Show the plots\n",
    "\n",
    "# Correlation matrix\n",
    "# Encoding categorical variables for correlation analysis\n",
    "df_encoded = df.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "for column in df_encoded.select_dtypes(include=['object']).columns:\n",
    "    df_encoded[column] = label_encoder.fit_transform(df_encoded[column])\n",
    "\n",
    "# Creating a heatmap for the correlation matrix of all columns\n",
    "plt.figure(figsize=(20, 20))\n",
    "correlation_matrix = df_encoded.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap for All Columns (After Label Encoding)')\n",
    "plt.show()  # Show the heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da2edf",
   "metadata": {},
   "source": [
    "Der Datensatz bietet relevante Informationen für das Machine-Learning-Projekt, indem er das Problem der Kreditvergabe anspricht. Obwohl einige Datenbereinigungen und -transformationen erforderlich sind, um die Vorhersagegenauigkeit des Modells zu verbessern und Auswirkungen wie reduzierte Modellleistung sowie verzerrte Modellschätzungen und Ergebnisse zu vermeiden, ist die Qualität der Daten insgesamt solide. Trotz des Vorhandenseins von Ausreißern und fehlenden Werten verfügt der Datensatz über ausreichend Datenpunkte, die potenziell bearbeitet werden können, um die Modellrobustheit zu steigern. Die Analyse liefert wichtige Erkenntnisse über die Einflussfaktoren auf die Kreditgenehmigung und legt den Grundstein für weitere Modellierungs- und Optimierungsschritte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72a125",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7eb797",
   "metadata": {},
   "source": [
    "Im Feature Engineering-Abschnitt wird zunächst die Datenbereinigung durchgeführt. Dieser Schritt ist wichtig, um sicherzustellen, dass der Datensatz frei von fehlenden Werten ist und eine konsistente Grundlage für die Modellbildung bietet. Für kategoriale Spalten werden fehlende Werte durch den am häufigsten vorkommenden Wert, den sogenannten Modus, ersetzt. Bei numerischen Spalten hingegen werden die fehlenden Werte durch den Median ersetzt, um die Auswirkungen von Ausreißern zu minimieren und eine robuste Schätzung zu gewährleisten. Diese Bereinigungsmethoden tragen dazu bei, dass der Datensatz homogen und für die folgenden Schritte des Feature Engineerings und der Modellierung geeignet ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with mode values for categorical columns\n",
    "for col in df.columns:\n",
    "    # Check if the data type of the column is 'object'\n",
    "    if df[col].dtype == 'object':\n",
    "        # For columns with 'object' data type: Convert to string data type and fill NaN values with mode\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Replace NaN values with median for numerical values\n",
    "for column in df.select_dtypes(include='number').columns:\n",
    "    # Calculate the median value of the column\n",
    "    median_value = df[column].median()\n",
    "    # Fill NaN values with the calculated median value\n",
    "    df[column].fillna(median_value, inplace=True)\n",
    "\n",
    "# Check if there are any remaining NaN values\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53866024",
   "metadata": {},
   "source": [
    "Anschließend werden basierend auf den vorhandenen Spalten neue Features generiert, wie zum Beispiel TotalIncome und die LoanAmount-to-TotalIncome-Ratio. Diese neuen Features können die Leistung der Modelle verbessern, indem sie zusätzliche Einblicke in die finanzielle Situation der Kreditnehmer liefern und möglicherweise relevante Muster hervorheben, die bei der Kreditentscheidung eine Rolle spielen könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'TotalIncome' by summing 'ApplicantIncome' and 'CoapplicantIncome'\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "\n",
    "# Create a new column 'LA2TI-Ratio' by dividing 'LoanAmount' by 'TotalIncome'\n",
    "df['LA2TI-Ratio'] = df['LoanAmount'] / df['TotalIncome']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d3845a",
   "metadata": {},
   "source": [
    "Im nächsten Schritt werden Ausreißer in den numerischen Spalten durch den Median ersetzt. Dieser Schritt ist wichtig, um sicherzustellen, dass extreme Werte das Modell nicht übermäßig beeinflussen und die Modellleistung verzerrt wird. Durch die Ersetzung von Ausreißern mit dem Median wird die Robustheit des Modells verbessert und eine zuverlässige Schätzung der Daten ermöglicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through numerical columns\n",
    "for column in df.select_dtypes(include='number').columns:\n",
    "    # Calculate the first quartile (Q1)\n",
    "    q1 = df[column].quantile(0.25)\n",
    "\n",
    "    # Calculate the third quartile (Q3)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "\n",
    "    # Calculate the interquartile range (IQR)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Calculate the lower bound\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "\n",
    "    # Calculate the upper bound\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    # Replace outliers with median value\n",
    "    df[column] = df[column].apply(lambda x: x if lower_bound <= x <= upper_bound else df[column].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5900f",
   "metadata": {},
   "source": [
    "Im nächsten Schritt werden die kategorialen Daten in ein maschinenlesbares Format mit der One-Hot-Encoding-Technik umgewandelt. Bei One-Hot-Encoding wird jede Kategorie in einen Binärvektor transformiert, wobei für jede Kategorie eine separate Dimension vorhanden ist. Diese Dimensionen werden mit Nullen initialisiert, außer derjenigen, die der Position der Kategorie im ursprünglichen Kategorie-Array entspricht, die den Wert Eins erhält.\n",
    "\n",
    "Ein einfaches Beispiel dafür ist die Umwandlung der \"Gender\"-Spalte. Durch One-Hot-Encoding werden zwei neue Spalten erstellt: \"gender_male\" und \"gender_female\". Wenn in der Spalte \"gender_male\" der Wert \"True\" ist, ist der Wert in der Spalte \"gender_female\" automatisch \"False\". Dieses Prinzip wird auf alle kategorialen Spalten angewendet. Es ist zu beachten, dass jeweils eine Spalte entfernt wird, um Redundanz zu vermeiden. Wenn beispielsweise eine Spalte zwei verschiedene Werte aufnehmen kann (z. B. \"Ja\" und \"Nein\"), reicht eine einzelne Binärspalte aus, um die Information zu repräsentieren.\n",
    "\n",
    "Die Umwandlung von Objektspalten in boolsche Spalten mittels One-Hot-Encoding ist wichtig, damit das Modell die kategorialen Daten korrekt interpretieren und nutzen kann. Maschinelle Lernalgorithmen, insbesondere lineare Modelle und Modelle, die auf numerischen Eingaben basieren, benötigen numerische Werte, um die Beziehungen zwischen den Merkmalen zu verstehen und Vorhersagen zu treffen. Kategoriale Daten in Form von Texten oder Wörtern können von den meisten Algorithmen nicht direkt verarbeitet werden. Daher ist es erforderlich, diese Daten in eine numerische Darstellung umzuwandeln, die das Modell verarbeiten kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f43f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of categorical columns\n",
    "categorical_columns = df.select_dtypes(include='object').columns\n",
    "\n",
    "# Perform One Hot Encoding on categorical columns\n",
    "# pd.get_dummies() function is used to convert categorical variables into dummy/indicator variables\n",
    "# 'drop_first=True' is used to drop the first category of each categorical variable to avoid multicollinearity issues\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Output information about the DataFrame after encoding\n",
    "df_encoded.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae6ffa",
   "metadata": {},
   "source": [
    "In diesem Schritt werden die Werte der numerischen Spalten normalisiert, sodass sie im Bereich zwischen 0 und 1 liegen. Dies dient dazu, die Merkmale auf vergleichbare Skalen anzupassen und Verzerrungen aufgrund unterschiedlicher Skalen zu vermeiden. Durch diese Maßnahme wird die potenzielle Probleme bei der Modellierung reduziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd15db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of numerical columns to be normalized\n",
    "numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'TotalIncome', 'LA2TI-Ratio']\n",
    "\n",
    "# Instantiate MinMaxScaler for normalization\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to the numerical columns using fit_transform()\n",
    "df_encoded[numerical_columns] = scaler.fit_transform(df_encoded[numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66872aa",
   "metadata": {},
   "source": [
    "Nach dem Encoding und der Normalisierung der Daten wurden nun zusätzliche Spalten mit einheitlicher Skalierung hinzugefügt, um die Informationen maschinenlesbar zu machen und potenzielle Probleme bei der Modellierung zu minimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output information about the DataFrame after encoding and normalization\n",
    "df_encoded.info()\n",
    "\n",
    "# Output descriptive statistics of the DataFrame after normalization\n",
    "# This provides summary statistics such as mean, min, max, etc. for each numerical column\n",
    "df_encoded.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad380d4d",
   "metadata": {},
   "source": [
    "# Split des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ea443",
   "metadata": {},
   "source": [
    "Nun erfolgt die Aufteilung des Datensatzes in Trainings-, Test- und Validierungssets. Zuerst wird der Datensatz in Trainings- und Testsets aufgeteilt, wobei 70% der Daten dem Trainingsset und 30% dem Testset zugewiesen werden. Anschließend wird das Testset erneut in Test- und Validierungssets aufgeteilt, wobei 60% des ursprünglichen Testsets dem neuen Testset und 40% dem Validierungsset zugewiesen werden.\n",
    "\n",
    "Nach der Durchführung des Splits ergibt sich folgende Größenverteilung der einzelnen Datensätze:\n",
    "- Trainingsset: 429 Einträgen\n",
    "- Validierungsset: 74 Einträgen\n",
    "- Testset: 111 Einträgen\n",
    "\n",
    "Diese Aufteilung garantiert, dass das Modell ausreichend Daten zum Training erhält, während separate Test- und Validierungssets zur Bewertung der Modellleistung genutzt werden können.\n",
    "\n",
    "Des Weiteren ist es wichtig zu berücksichtigen, dass Ungleichgewichte im Datensatz bei einer zufälligen Aufteilung zu einer verzerrten Stichprobe führen können, insbesondere wenn die Klassen unterschiedlich häufig vertreten sind. In solchen Situationen wird eine stratifizierte Aufteilung des Datensatzes durchgeführt, um sicherzustellen, dass die Proportionen jeder Klasse sowohl im Trainings- als auch im Testset gleich bleiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into features (X) and target variable (y)\n",
    "X = df_encoded.drop('Loan_Status_Y', axis=1)  # Features\n",
    "y = df_encoded['Loan_Status_Y']  # Target variable\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7,stratify=y)\n",
    "\n",
    "# Further split the test set into test and validation sets\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.4, random_state=7)\n",
    "\n",
    "# Print the sizes of the individual sets\n",
    "print(\"Größe des Trainingssets:\", X_train.shape[0])\n",
    "print(\"Größe des Validierungssets:\", X_val.shape[0])\n",
    "print(\"Größe des Testsets:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559c9e4b114307d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hier wird das Ungleichgewicht in den Klassifikationsdatensätzen mit der SMOTE-Technik (Synthetic Minority Over-sampling Technique) behoben. Diese Methode vergrößert die Minderheitsklasse im Trainingsdatensatz durch die Generierung synthetischer Beispiele, was drei wesentliche Vorteile bietet:\n",
    "\n",
    "Erstens verhindert SMOTE Verzerrungen, indem sichergestellt wird, dass das Modell die Minderheitsklasse angemessen lernt und sich nicht ausschließlich auf die Mehrheitsklasse konzentriert. Dies ist besonders wichtig, wenn das Hauptinteresse auf der Minderheitsklasse liegt. Zweitens trägt SMOTE dazu bei, Overfitting zu reduzieren, da durch die Erweiterung des Trainingssets das Risiko gemindert wird, dass das Modell zu spezifisch auf die Mehrheitsklasse zugeschnitten wird. Drittens verbessert die SMOTE-Technik die Gesamtleistung des Modells, indem sie der Minderheitsklasse mehr Gewicht verleiht und dem Modell mehr Trainingsbeispiele dieser Klasse zur Verfügung stellt. Dies führt zu einem ausgewogeneren Verhältnis zwischen den Klassen und ermöglicht genauere Vorhersagen bei seltenen Ereignissen.\n",
    "\n",
    "Vor der Anwendung von SMOTE war die Verteilung der Klassen im Trainingsset unausgewogen: {1:295, 0:134}.\n",
    "Nach dem Einsatz von SMOTE zeigt die Verteilung ein ausgeglichenes Verhältnis: {1: 295, 0: 295}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ac03d2400a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the distribution of classes before applying SMOTE\n",
    "print(\"Verteilung der Klassen in training set vor Oversampling:\", Counter(y_train))\n",
    "\n",
    "# Initialize the SMOTE oversampler\n",
    "smote = SMOTE(random_state=7)\n",
    "\n",
    "# Apply SMOTE to the data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the distribution of classes after applying SMOTE\n",
    "print(\"Verteilung der Klassen in training set nach SMOTE:\", Counter(y_train_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168a639",
   "metadata": {},
   "source": [
    "Der Trainingsdatensatz wird durch das SMOTE-Verfahren ausbalanciert, wobei Validierungs- und Testdatensätze unverändert bleiben. Dies geschieht aus dem Grund, dass diese Datensätze die natürliche Verteilung der Klassen widerspiegeln sollen, um realistisch zu bewerten, wie gut das Modell mit neuen, unbekannten Daten umgehen kann. Das Anwenden von SMOTE nur auf den Trainingsdatensatz verhindert eine künstliche Verzerrung der Modellbewertung, da das Ziel darin besteht, die Leistung des Modells unter realen Bedingungen zu testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of instances of each class in each set\n",
    "train_classes, train_counts = np.unique(y_train_resampled, return_counts=True)\n",
    "test_classes, test_counts = np.unique(y_test, return_counts=True)\n",
    "val_classes, val_counts = np.unique(y_val, return_counts=True)\n",
    "\n",
    "# Calculate the percentage distribution of classes in each set\n",
    "train_class_percentages = {cls: count / len(y_train_resampled) * 100 for cls, count in zip(train_classes, train_counts)}\n",
    "test_class_percentages = {cls: count / len(y_test) * 100 for cls, count in zip(test_classes, test_counts)}\n",
    "val_class_percentages = {cls: count / len(y_val) * 100 for cls, count in zip(val_classes, val_counts)}\n",
    "\n",
    "# Display the percentage distribution of classes for each set\n",
    "print(\"Percentage class distribution in the training set:\", train_class_percentages)\n",
    "print(\"Percentage class distribution in the validation set:\", val_class_percentages)\n",
    "print(\"Percentage class distribution in the test set:\", test_class_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bfc48",
   "metadata": {},
   "source": [
    "# Auswahl der Metriken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0e715",
   "metadata": {},
   "source": [
    "Für eine Bank, die über die Vergabe von Krediten entscheidet, ist die Bewertung der Kreditvergabe ein sensibler Prozess, der sorgfältige Überlegungen erfordert. In einem solchen Kontext sind Metriken wie Precision, Recall und F1-Score von besonderer Relevanz.\n",
    "\n",
    "Precision misst das Verhältnis der korrekt als kreditwürdig identifizierten Kreditnehmer zur Gesamtanzahl der als kreditwürdig identifizierten Kreditnehmer. Ein hoher Precision-Wert deutet darauf hin, dass das Modell eine genauere Auswahl potenziell kreditwürdiger Kunden trifft, was das Risiko von Kreditausfällen verringert, indem es nur Kunden auswählt, von denen angenommen wird, dass sie ihre Kredite zurückzahlen können. Ein hoher Recall-Wert bedeutet, dass das Modell dazu neigt, die meisten potenziell kreditwürdigen Kunden zu identifizieren. Dies könnte der Bank helfen, potenzielle Kunden nicht zu verlieren und ihr Kreditgeschäft zu erweitern.\n",
    "Die Wahl zwischen Recall und Precision hängt davon ab, welche Art von Fehlern die Bank eher vermeiden möchte. Wenn die Bank eher sicherstellen möchte, dass sie keinen potenziell kreditwürdigen Kunden verpasst, selbst wenn dies bedeutet, einige nicht kreditwürdige Kunden zu akzeptieren (hoher Recall, geringe Precision), wäre Recall die bevorzugte Metrik. In diesem Fall ist es der Bank wichtiger, möglichst viele kreditwürdige Kunden zu identifizieren, auch wenn dadurch einige nicht kreditwürdige Kunden fälschlicherweise akzeptiert werden.\n",
    "Wenn die Bank jedoch das Risiko von Kreditausfällen minimieren möchte, selbst wenn dies bedeutet, potenziell kreditwürdige Kunden abzulehnen (hohe Precision, geringer Recall), wäre Precision die bevorzugte Metrik. Hier liegt der Fokus darauf, nur Kunden zu akzeptieren, bei denen das Modell sehr sicher ist, dass sie kreditwürdig sind, auch wenn dadurch einige tatsächlich kreditwürdige Kunden abgelehnt werden.\n",
    "\n",
    "Im vorliegenden Datensatz wurden jedoch deutlich mehr Kredite genehmigt als abgelehnt. Dies könnte darauf hinweisen, dass die Kosten für das Ablehnen von kreditwürdigen Kunden höher sind als die Kosten von Kreditausfällen oder dass die Bank ein breiteres Risiko eingehen möchte.\n",
    "\n",
    "In unserem Fall könnte man sich jedoch dafür entscheiden, auf den F1-Score zu optimieren, denn die Kosten der jeweiligen Entscheidungen und die wirtschaftliche Situation sind hier unklar. Der F1-Score wird maximiert, wenn Recall und Precision gleich sind, was bedeutet, dass das Modell eine ausgewogene Bewertung zwischen beiden Metriken bietet. Der F1-Score berücksichtigt sowohl False Positives als auch False Negatives und wäre daher eine sinnvolle Entscheidung für die Modellbewertung, wenn die Bank ein ausgewogenes Verhältnis zwischen der Identifizierung kreditwürdiger Kunden und der Minimierung von Kreditausfällen anstrebt.\n",
    "\n",
    "Es sind zwar weitere Metriken wie Accuracy oder ROC AUC verfügbar, berücksichtigen diese nicht spezifisch die Kosten von Fehlklassifikationen. In einem Szenario, in dem False Positives und False Negatives unterschiedliche Auswirkungen haben können, sind Metriken wie Precision und Recall besser geeignet, um die Leistung des Modells zu bewerten. Wenn die Kosten für die Entscheidungen der Bank, ob sie einem Kreditnehmer einen Kredit gewährt oder nicht, nicht bekannt sind oder nicht berücksichtigt werden können, ist es schwierig, eine präzise Bewertung der Modelleffizienz vorzunehmen. In solchen Fällen könnte es angemessen sein, auf Metriken wie Accuracy oder ROC AUC zu verzichten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423d4ab",
   "metadata": {},
   "source": [
    "# Auswahl und Beschreibung der ML-Methode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753b8ae",
   "metadata": {},
   "source": [
    "Für die Vorhersage wurde das Random Forest-Modell ausgewählt, da es sich als gute Wahl für die binäre Klassifikation erweist, insbesondere wenn ein Großteil der Features kategorial ist. Die Robustheit gegen Overfitting macht es zu einer attraktiven Option, da es eine zuverlässige Generalisierung auf neue Daten ermöglicht. Zudem ist Random Forest in der Lage, sowohl mit numerischen als auch mit kategorialen Variablen effektiv umzugehen, was die Flexibilität des Modells erhöht. Im Vergleich zu einigen anderen Modellen erfordert Random Forest weniger Feinabstimmung der Hyperparameter, was die Modellentwicklung und -optimierung erleichtert.\n",
    "\n",
    "Random Forest funktioniert, indem es eine Vielzahl von Entscheidungsbäumen erstellt, die jeweils auf einem zufälligen Teil des Trainingsdatensatzes trainiert werden. Jeder Baum gibt eine Vorhersage ab, und die Vorhersage des Random Forest-Modells wird durch Durchschnitt oder Mehrheitsabstimmung der Vorhersagen der einzelnen Bäume bestimmt. Dies ermöglicht eine robuste und stabile Vorhersageleistung, da das Modell durch die Kombination mehrerer Bäume weniger anfällig für Rauschen und Overfitting ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest Classifier model with a fixed random state for reproducibility\n",
    "rf_model = RandomForestClassifier(random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae37c0",
   "metadata": {},
   "source": [
    "# Implementierung Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52fa28",
   "metadata": {},
   "source": [
    "Der Code rf_model.fit(X_train_resampled, y_train_resampled) trainiert das Random Forest-Modell. Dabei werden die durch SMOTE-Verfahren modifizierte Trainingsdaten X_train_resampled verwendet, die die Merkmale (Features) enthalten, und die entsprechenden Zielvariablen y_train_resampled, die die Labels oder Klassen darstellen. Während des Trainings passt das Modell eine Vielzahl von Entscheidungsbäumen an die Trainingsdaten an. Jeder Baum wird auf einem zufälligen Teil des Trainingsdatensatzes trainiert, und während des Trainings optimiert das Modell die Entscheidungsregeln in jedem Baum, um die Vorhersagegenauigkeit zu maximieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee21dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest Classifier model using the training data\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da521a",
   "metadata": {},
   "source": [
    "Basierend auf den präsentierten Ergebnissen kann festgestellt werden, dass die Leistung des Modells bei der Bewertung der Kreditvergabe als mäßig einzustufen ist. Der ROC-AUC-Wert von 0,6368286445012787 deutet darauf hin, dass die Trennschärfe zwischen kreditwürdigen und nicht kreditwürdigen Kunden noch Verbesserungspotenzial aufweist.\n",
    "\n",
    "Im Klassifikationsbericht wird ersichtlich, dass das Modell eine höhere Precision von 0,76 für die Klasse der kreditwürdigen Kunden aufweist, was bedeutet, dass 76% der als kreditwürdig eingestuften Kunden tatsächlich kreditwürdig sind. Allerdings ist der Recall-Wert für diese Klasse mit 0,88 deutlich höher, was darauf hindeutet, dass das Modell dazu neigt, die meisten potenziell kreditwürdigen Kunden zu identifizieren, selbst wenn dies mit dem Risiko verbunden ist, einige nicht kreditwürdige Kunden als kreditwürdig einzustufen. Für die Klasse der nicht kreditwürdigen Kunden zeigt sich ein umgekehrtes Bild: Hier ist der Precision-Wert mit 0,60 niedriger, während der Recall-Wert mit 0,39 relativ gering ausfällt. Dies könnte darauf hinweisen, dass das Modell Schwierigkeiten hat, nicht kreditwürdige Kunden zuverlässig zu identifizieren. Berücksichtigt man die ungleiche Verteilung der Klassen im Validierungsdatensatz (31,08% Klasse 0, 68,92% Klasse 1), könnte dies eine mögliche Erklärung für die niedrigeren Werte bei der Erkennung von nicht kreditwürdigen Kunden sein. Das Modell könnte dazu tendieren, Kunden eher als kreditwürdig einzustufen, um mögliche Geschäftsverluste zu vermeiden.\n",
    "\n",
    "Der gesamte F1-Score von 0,8181818181818181 für die Validierungsdaten kann als akzeptable Leistung des Modells interpretiert werden. Allerdings zeigt sich, dass je nach Risikobereitschaft und strategischen Prioritäten der Bank, entweder der Precision-Wert (um das Kreditausfallrisiko zu senken) oder der Recall-Wert (um mehr potenzielle Kunden zu akquirieren) optimiert werden könnte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the model on validation data\n",
    "# Predictions on validation data using the trained Random Forest Classifier model\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "print(\"RF-Classifier:\")\n",
    "# Output ROC-AUC for validation data\n",
    "print(\"ROC-AUC for validation data:\", roc_auc_score(y_val, y_val_pred))\n",
    "\n",
    "# Output classification report for validation data\n",
    "print(\"Classification Report for validation data:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and output the F1-score for validation data average=binary(None)\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "print(\"F1-Score for validation data:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Set the figure size for the visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Define F1-Score as the scoring function for the learning curve\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Learning Curve\n",
    "# Generate the learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(rf_model, X_train_resampled, y_train_resampled, cv=5, scoring=scorer)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', color=\"g\", label=\"Validation score\")\n",
    "plt.xlabel('Training Examples')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5fb8312d128bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Die Grafik zur Feature-Wichtigkeit zeigt, dass das Feature CreditHistory das entscheidendste für die Vorhersage ist, gefolgt von den fünf numerischen Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc34b16bfa1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Feature Importance \n",
    "feature_names = X_train_resampled.columns.tolist()\n",
    "importances = rf_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# visualize Feature Importance \n",
    "plt.figure()\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.barh(range(X_train_resampled.shape[1]), importances[indices], color=\"r\", xerr=std[indices], align=\"center\")\n",
    "plt.yticks(range(X_train_resampled.shape[1]), [feature_names[i] for i in indices], fontsize= 10)\n",
    "plt.ylim([-1, X_train_resampled.shape[1]])\n",
    "plt.ylabel('Feature Name')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283ed47",
   "metadata": {},
   "source": [
    "# Hyperparametertuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b897968",
   "metadata": {},
   "source": [
    "Dieser Schritt führt ein Hyperparameter-Tuning für das Random Forest-Modell durch, indem Grid-Search verwendet wird. GridSearchCV ist eine Methode zur Durchführung von Hyperparameter-Tuning, bei der verschiedene Kombinationen von Hyperparametern durchlaufen und bewertet werden, um die besten Hyperparameter für das Modell zu finden und die Leistung zu optimieren.\n",
    "\n",
    "In diesem Fall wird GridSearchCV mit dem Random Forest Classifier-Modell, dem definierten Hyperparameter-Grid, einer 3-fachen Kreuzvalidierung, dem F1-Score als Bewertungsmetrik und paralleler Verarbeitung instanziiert. Der ursprüngliche Parameter-Grid wurde auskommentiert, um die Laufzeit zu verkürzen. Daher wird der zweite Grid mit den besten Parametern verwendet, die im vorherigen Schritt gefunden wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29038b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grid for GridSearchCV\n",
    "# Define a grid of hyperparameters to search over\n",
    "# param_grid = {\n",
    "#     'bootstrap': [True,False],\n",
    "#     'class_weight': [{0: 1, 1: 1}, {0: 1.61, 1: 1}, 'balanced'],\n",
    "#     'max_depth': [5, 25 , 75, None], \n",
    "#     'max_features': ['auto', 'sqrt'],  \n",
    "#     'min_samples_leaf': [1, 2, 5, 25],  \n",
    "#     'min_samples_split': [2, 5, 10,25],\n",
    "#     'n_estimators': [50, 100, 150, 300, 500,1000], \n",
    "#     'random_state': [1,4,7,16,64] \n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'class_weight': [{0: 1, 1: 1}],\n",
    "    'max_depth': [25],  \n",
    "    'max_features': ['auto'],  \n",
    "    'min_samples_leaf': [1],  \n",
    "    'min_samples_split': [2], \n",
    "    'n_estimators': [300], \n",
    "    'random_state': [7],\n",
    "}\n",
    "\n",
    "# GridSearchCV for Hyperparameter Tuning\n",
    "# Instantiate GridSearchCV with the Random Forest Classifier model, the hyperparameter grid, 3-fold cross-validation, F1 scoring, verbose mode, and parallel processing\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=3, scoring='f1', verbose=3, n_jobs=-1)\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Display the best hyperparameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3926887",
   "metadata": {},
   "source": [
    "# Evaluation und Ergebnisdarstellung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820782f6",
   "metadata": {},
   "source": [
    "Das optimierte Random Forest-Modell zur Bewertung der Kreditwürdigkeit zeigt insgesamt eine mäßige Verbesserung der Leistungsfähigkeit im Vergleich zum ursprünglichen Modell.\n",
    "\n",
    "Der ROC-AUC-Wert des Testdatensatzes liegt nun bei 0,684. Dieser Wert deutet darauf hin, dass die Trennschärfe des Modells zwischen kreditwürdigen und nicht kreditwürdigen Kunden als mäßig einzustufen ist. Es gibt also noch Potenzial für weitere Verbesserungen. Im Klassifikationsbericht für den Testdatensatz wird ersichtlich, dass das Modell eine Precision von 0,79 für die Klasse der kreditwürdigen Kunden aufweist. Das bedeutet, dass 79% der als kreditwürdig eingestuften Kunden tatsächlich kreditwürdig sind. Der Recall-Wert für diese Klasse liegt bei 0,88, was darauf hindeutet, dass das Modell weiterhin dazu neigt, die meisten potenziell kreditwürdigen Kunden zu identifizieren. Für die Klasse der nicht kreditwürdigen Kunden zeigt sich ein Precision-Wert von 0,65 und ein Recall-Wert von 0,49. Dies bedeutet, dass das Modell Schwierigkeiten hat, nicht kreditwürdige Kunden zuverlässig zu erkennen. Hier besteht weiterhin Optimierungspotenzial. Der F1-Score für die Klasse der kreditwürdigen Kunden liegt bei sehr guten 0,83. Für die Klasse der nicht kreditwürdigen Kunden beträgt der F1-Score 0,56, was auf Verbesserungspotenzial hindeutet.\n",
    "\n",
    "Bemerkenswert ist, dass der F1-Score über beide Klassen hinweg bei 0,832 liegt. Dies zeigt, dass das optimierte Modell insgesamt eine sehr gute Leistung erbringt. Insgesamt lässt sich festhalten, dass das optimierte Random Forest-Modell eine deutliche Verbesserung der Leistungsfähigkeit im Vergleich zum vorherigen Modell aufweist. Zwar besteht weiterhin Potenzial, um die Vorhersagegenauigkeit, insbesondere für die Klasse der nicht kreditwürdigen Kunden, zu erhöhen, aber das Modell bietet bereits eine solide Grundlage für die Unterstützung der Kreditvergabeentscheidungen. Je nach Risikobereitschaft und strategischen Prioritäten der Bank könnte eine weitere Optimierung des Modells sinnvoll sein, um ein noch ausgewogeneres Verhältnis zwischen Precision und Recall zu erreichen.\n",
    "\n",
    "Um die Leistung des Modells weiter zu verbessern, könnten folgende Ansätze in Betracht gezogen werden:\n",
    "1. Verbesserung der Datenqualität\n",
    "2. Verwendung fortgeschrittener Techniken wie Ensemble-Methoden (z.B. Bagging, Boosting):\n",
    "3. Ausprobieren anderer Modelltypen (z.B. logistische Regression, SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best Random Forest model from GridSearchCV\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluation of the model on test data\n",
    "# Predictions on test data using the best Random Forest model\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "print(\"tuned RF-Classifier:\")\n",
    "# Output ROC-AUC for test data\n",
    "print(\"ROC-AUC for test data:\", roc_auc_score(y_test, y_test_pred))\n",
    "\n",
    "# Output classification report for test data\n",
    "print(\"Classification Report for test data:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Calculate and output the F1-score for test data\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"F1-Score for test data:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4ad56",
   "metadata": {},
   "source": [
    "Hier wurde die Vorhersage erneut mit einer Confusion Matrix dargestellt. Insgesamt hat das Modell gute Vorhersagen getroffen. \n",
    "\n",
    "Auch die Lernkurve wird angezeigt. Es ist zu erkennen, dass die Steigung tendenziell steigend ist. Mit einem größeren Datensatz könnte daher auch eine verbesserte Leistung des Modells erwartet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78faecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Set the figure size for the visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Confusion Matrix for y_test and y_test_pred\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Define F1-Score as the scoring function for the learning curve\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Learning Curve\n",
    "# Generate the learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(best_rf_model, X_train_resampled, y_train_resampled, cv=5, scoring=scorer)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', color=\"g\", label=\"Test score\")\n",
    "plt.xlabel('Training Examples')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29509326",
   "metadata": {},
   "source": [
    "Der Dummy-Classifier wird als einfaches Klassifikationsmodell verwendet, um als Basislinie zur Bewertung der Leistung anderer Modelle zu dienen. Es werden Vorhersagen basierend auf einfachen Regeln oder einer vorgegebenen Strategie generiert, ohne dass tatsächlich Muster oder Beziehungen in den Daten erlernt werden. Die Strategie 'stratified' des Dummy-Classifiers wird angewendet, um die Klassenverteilung im Trainingsdatensatz nachzuahmen. Die Leistung des Dummy-Classifiers zeigt eine relativ gleichmäßige Verteilung zwischen den Klassen, was zu einer niedrigen Genauigkeit von 51,35% und einem F1-Score von 0,597 führt. Die ROC-AUC liegt ebenfalls nahe bei 0,51, was darauf hinweist, dass der Classifier nicht in der Lage ist, zwischen den Klassen zu unterscheiden und zufällige Vorhersagen macht. Im Vergleich dazu zeigt das zuvor beschriebene, optimierte Modell eine deutlich verbesserte Leistung, wie durch eine höhere Genauigkeit, einen höheren F1-Score und eine höhere ROC-AUC gekennzeichnet ist. Dies deutet darauf hin, dass das optimierte Modell über die zufälligen Vorhersagen des Dummy-Classifiers hinausgeht und tatsächlich Muster und Zusammenhänge in den Daten lernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Classifier\n",
    "# Initialize a Dummy Classifier with 'stratified' strategy and a fixed random state\n",
    "dummy_clf = DummyClassifier(strategy='stratified', random_state=7)\n",
    "\n",
    "# Train the Dummy Classifier on the training data\n",
    "dummy_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions on test data using the Dummy Classifier\n",
    "y_test_dummy = dummy_clf.predict(X_test)\n",
    "\n",
    "print(\"Dummy Classifier (stratified):\")\n",
    "# Output ROC-AUC for test data\n",
    "print(\"ROC-AUC for test data:\", roc_auc_score(y_test, y_test_dummy))\n",
    "\n",
    "# Output classification report for test data\n",
    "print(\"Classification Report for test data:\")\n",
    "print(classification_report(y_test, y_test_dummy))\n",
    "\n",
    "# Calculate and output the F1-score for test data\n",
    "f1 = f1_score(y_test, y_test_dummy)\n",
    "print(\"F1-Score for test data:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd66cd",
   "metadata": {},
   "source": [
    "# Vorhersage-Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d214240",
   "metadata": {},
   "source": [
    "Für die Vorhersage wurde eine Demonstration mit dem getunten Random Forest-Modell durchgeführt, das durch Gridsearch optimiert wurde. Die Vorhersage wurde mit dem Testdatensatz durchgeführt. Das Ergebnis zeigt, dass das Modell in 4 von 5 Fällen richtig lag, was darauf hindeutet, dass es in der Lage ist, eine gute Schätzung abzugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f854b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose five random indices from the test dataset\n",
    "random.seed(9)  # Set the random seed for reproducibility\n",
    "indices = random.sample(range(len(X_test)), 5)\n",
    "\n",
    "# Iterate over the selected indices and make predictions for each index\n",
    "for index in indices:\n",
    "    # Select the data for the current index\n",
    "    X_sample = X_test.iloc[[index]]\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = best_rf_model.predict(X_sample)\n",
    "    \n",
    "    # Display the prediction and the actual value\n",
    "    print(\"Index:\", X_sample.index[0])\n",
    "    print(\"Prediction:\", prediction[0])\n",
    "    print(\"Actual Value:\", y_test.iloc[index])\n",
    "    print(\"-------------------------\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
